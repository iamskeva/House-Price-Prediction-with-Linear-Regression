{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "leading-boundary",
   "metadata": {},
   "source": [
    "# House Price Prediction with Linear Regression\n",
    "\n",
    "![](https://i.imgur.com/3sw1fY9.jpg)\n",
    "\n",
    "We'll be predicting the price of a house using information like its location, area, no. of rooms etc. we'll use the dataset from the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition on [Kaggle](https://kaggle.com). We'll follow a step-by-step process to train our model:\n",
    "\n",
    "1. Download and explore the data\n",
    "2. Prepare the dataset for training\n",
    "3. Train a linear regression model\n",
    "4. Make predictions and evaluate the model\n",
    "\n",
    "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end and your machine learning model is trained properly without errors. \n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "1. Make sure to run all the code cells in order. Otherwise, you may get errors like `NameError` for undefined variables.\n",
    "2. Do not change variable names, delete cells, or disturb other existing code. It may cause problems during evaluation.\n",
    "3. In some cases, you may need to add some code cells or new statements before or after the line of code containing the **???**. \n",
    "4. Since you'll be using a temporary online service for code execution, save your work by running `jovian.commit` at regular intervals.\n",
    "5. Review the \"Evaluation Criteria\" for the assignment carefully and make sure your submission meets all the criteria.\n",
    "6. Questions marked **(Optional)** will not be considered for evaluation and can be skipped. They are for your learning.\n",
    "7. It's okay to ask for help & discuss ideas on the [community forum](https://jovian.ai/forum/c/zero-to-gbms/gbms-assignment-1/100), but please don't post full working code, to give everyone an opportunity to solve the assignment on their own.\n",
    "\n",
    "\n",
    "**Important Links**:\n",
    "\n",
    "- Make a submission here: https://jovian.ai/learn/machine-learning-with-python-zero-to-gbms/assignment/assignment-1-train-your-first-ml-model\n",
    "- Ask questions, discuss ideas and get help here: https://jovian.ai/forum/c/zero-to-gbms/gbms-assignment-1/100\n",
    "- Review the following notebooks:\n",
    "    - https://jovian.ai/aakashns/python-sklearn-linear-regression\n",
    "    - https://jovian.ai/aakashns/python-sklearn-logistic-regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-preparation",
   "metadata": {},
   "source": [
    "## How to Run the Code and Save Your Work\n",
    "\n",
    "\n",
    "**Option 1: Running using free online resources (1-click, recommended):** The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Binder**. This will set up a cloud-based Jupyter notebook server and allow you to modify/execute the code.\n",
    "\n",
    "\n",
    "**Option 2: Running on your computer locally:** To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions.\n",
    "\n",
    "**Saving your work**: You can save a snapshot of the assignment to your [Jovian](https://jovian.ai) profile, so that you can access it later and continue your work. Keep saving your work by running `jovian.commit` from time to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "undefined-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian scikit-learn --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stainless-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-pitch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit(project='House Price Prediction with Linear Regression', privacy='secret')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-teaching",
   "metadata": {},
   "source": [
    "Let's begin by installing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn plotly opendatasets jovian --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-inside",
   "metadata": {},
   "source": [
    "## Step 1 - Download and Explore the Data\n",
    "\n",
    "The dataset is available as a ZIP file at the following url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://github.com/JovianML/opendatasets/raw/master/data/house-prices-advanced-regression-techniques.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-output",
   "metadata": {},
   "source": [
    "We'll use the `urlretrieve` function from the module [`urllib.request`](https://docs.python.org/3/library/urllib.request.html) to dowload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve(dataset_url, 'house-prices.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-cardiff",
   "metadata": {},
   "source": [
    "The file `housing-prices.zip` has been downloaded. Let's unzip it using the [`zipfile`](https://docs.python.org/3/library/zipfile.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('house-prices.zip') as f:\n",
    "    f.extractall(path='house-prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-auction",
   "metadata": {},
   "source": [
    "The dataset is extracted to the folder `house-prices`. Let's view the contents of the folder using the [`os`](https://docs.python.org/3/library/os.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'house-prices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-tulsa",
   "metadata": {},
   "source": [
    "Use the \"File\" > \"Open\" menu option to browse the contents of each file. You can also check out the [dataset description](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) on Kaggle to learn more.\n",
    "\n",
    "We'll use the data in the file `train.csv` for training our model. We can load the for processing using the [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = data_dir + '/train.csv'\n",
    "train_csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-deployment",
   "metadata": {},
   "source": [
    "> Loading the data from the file `train.csv` into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = pd.read_csv(train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-oxide",
   "metadata": {},
   "source": [
    "Let's explore the columns and data types within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-tomorrow",
   "metadata": {},
   "source": [
    "> Counting the rows and columns in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(prices_df.index)\n",
    "n_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(prices_df.columns)\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset contains {} rows and {} columns.'.format(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f17c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='OverallQual', y='SalePrice', opacity=0.65,\n",
    "    trendline='ols', trendline_color_override='red', title='Relationship between the Overall Quality of House and House Sales Price ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='OverallCond', y='SalePrice', opacity=0.65,\n",
    "    trendline='ols', trendline_color_override='black', title='Relationship between the Overall Condition of House and House Sales Price ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='YearBuilt', y='SalePrice', opacity=0.65,\n",
    "    trendline='ols', trendline_color_override='black', title='Relationship between the Year House was Built & House Sales Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='YearRemodAdd', y='SalePrice', opacity=0.65,\n",
    "    trendline='ols', trendline_color_override='red', title='Relationship between House Remodel Date & House Sales Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='YearBuilt', y='SalePrice', \n",
    "           opacity=0.65, \n",
    "           color='YearRemodAdd', \n",
    "           title='How the Year House was Remodelled affects the Price of House')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='YearBuilt', y='SalePrice',\n",
    "           opacity=0.65, \n",
    "           color='Neighborhood',\n",
    "           title='How Location of House Affects The House Sales Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d1066",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "px.scatter(data_frame=prices_df, x='YearBuilt', y='SalePrice',\n",
    "           opacity=0.65, \n",
    "           color='HouseStyle',\n",
    "           title='How The House Style Affects The House Sales Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-civilian",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-ceremony",
   "metadata": {},
   "source": [
    "## Step 2 - Prepare the Dataset for Training\n",
    "\n",
    "Before we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n",
    "\n",
    "1. Identify the input and target column(s) for training the model.\n",
    "2. Identify numeric and categorical input columns.\n",
    "3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns\n",
    "4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.\n",
    "5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.\n",
    "6. Split the dataset into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-organ",
   "metadata": {},
   "source": [
    "### Identify Inputs and Targets\n",
    "\n",
    "While the dataset contains 81 columns, not all of them are useful for modeling. Note the following:\n",
    "\n",
    "- The first column `Id` is a unique ID for each house and isn't useful for training the model.\n",
    "- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.\n",
    "- Data from all the other columns (except the first and the last column) can be used as inputs to the model.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-regression",
   "metadata": {},
   "source": [
    "> Creating a list `input_cols` of column names containing data that can be used as input to train the model, and identify the target column as the variable `target_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the input columns (a list of column names)\n",
    "input_cols = list(prices_df.iloc[:, 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the name of the target column (a single string, not a list)\n",
    "target_col = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(input_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-border",
   "metadata": {},
   "source": [
    "Make sure that the `Id` and `SalePrice` columns are not included in `input_cols`.\n",
    "\n",
    "Now that we've identified the input and target columns, we can separate input & target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df = prices_df[input_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = prices_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-distributor",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-conclusion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-adjustment",
   "metadata": {},
   "source": [
    "### Identify Numeric and Categorical Data\n",
    "\n",
    "The next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-afternoon",
   "metadata": {},
   "source": [
    "> Creating two lists `numeric_cols` and `categorical_cols` containing names of numeric and categorical input columns within the data set respectively. Numeric columns have data types `int64` and `float64`, whereas categorical columns have the data type `object`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = inputs_df.select_dtypes(include = ['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(numeric_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(categorical_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-small",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-acquisition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-organ",
   "metadata": {},
   "source": [
    "### Impute Numerical Data\n",
    "\n",
    "Some of the numeric columns in our dataset contain missing values (`nan`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\n",
    "missing_counts[missing_counts > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-personality",
   "metadata": {},
   "source": [
    "Machine learning models can't work with missing data. The process of filling missing values is called [imputation](https://scikit-learn.org/stable/modules/impute.html).\n",
    "\n",
    "<img src=\"https://i.imgur.com/W7cfyOp.png\" width=\"480\">\n",
    "\n",
    "There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-animal",
   "metadata": {},
   "source": [
    "> **QUESTION 5**: Impute (fill) missing values in the numeric columns of `inputs_df` using a `SimpleImputer`. \n",
    ">\n",
    "> *Hint*: See [this notebook](https://jovian.ai/aakashns/python-sklearn-logistic-regression/v/66#C88)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the imputer\n",
    "imputer = SimpleImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fitting the imputer to the numeric colums\n",
    "imputer.fit(inputs_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transforming and replacing the numeric columns\n",
    "inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-graduate",
   "metadata": {},
   "source": [
    "After imputation, none of the numeric columns should contain any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\n",
    "missing_counts[missing_counts > 0] # should be an empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-summer",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-japanese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-brand",
   "metadata": {},
   "source": [
    "### Scale Numerical Values\n",
    "\n",
    "The numeric columns in our dataset have varying ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-nudist",
   "metadata": {},
   "source": [
    "A good practice is to [scale numeric features](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-sector",
   "metadata": {},
   "source": [
    "> Scaling numeric values to the $(0, 1)$ range using `MinMaxScaler` from `sklearn.preprocessing`.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler to the numeric columns\n",
    "model=scaler.fit(inputs_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and replace the numeric columns\n",
    "inputs_df[numeric_cols] = model.transform(inputs_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-iraqi",
   "metadata": {},
   "source": [
    "After scaling, the ranges of all numeric columns should be $(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-congress",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-screening",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-result",
   "metadata": {},
   "source": [
    "### Encode Categorical Columns\n",
    "\n",
    "Our dataset contains several categorical columns, each with a different number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df[categorical_cols].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-genesis",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n",
    "\n",
    "<img src=\"https://i.imgur.com/n8GuiOO.png\" width=\"640\">\n",
    "\n",
    "One hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-italic",
   "metadata": {},
   "source": [
    "> Encoding the categorical columns in the dataset as one-hot vectors using `OneHotEncoder` from `sklearn.preprocessing`. Add a new binary (0/1) column for each category\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the encoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fitting the encoder to the categorical colums\n",
    "encoder.fit(inputs_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generating column names for each category\n",
    "encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
    "len(encoded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transform and add new one-hot category columns\n",
    "inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-cigarette",
   "metadata": {},
   "source": [
    "The new one-hot category columns should now be added to `inputs_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-geology",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-outreach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-finnish",
   "metadata": {},
   "source": [
    "### Training and Validation Set\n",
    "\n",
    "Finally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cleaned data set as the datset for training our model\n",
    "train_df = inputs_df[numeric_cols + encoded_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(train_df, \n",
    "                                                                        targets, \n",
    "                                                                        test_size=0.25, \n",
    "                                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-prior",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-factor",
   "metadata": {},
   "source": [
    "## Step 3 - Train a Linear Regression Model\n",
    "\n",
    "We're now ready to train the model. Linear regression is a commonly used technique for solving regression problems. \n",
    "\n",
    "In a linear regression model, the target is modeled as a linear combination (or weighted sum) of input features. The predictions from the model are evaluated using a loss function like the Root Mean Squared Error (RMSE).\n",
    "\n",
    "\n",
    "Here's a visual summary of how a linear regression model is structured:\n",
    "\n",
    "<img src=\"https://i.imgur.com/iTM2s5k.png\" width=\"480\">\n",
    "\n",
    "However, linear regression doesn't generalize very well when we have a large number of input columns with co-linearity i.e. when the values one column are highly correlated with values in other column(s). This is because it tries to fit the training data perfectly. \n",
    "\n",
    "Instead, we'll use Ridge Regression, a variant of linear regression that uses a technique called L2 regularization to introduce another loss term that forces the model to generalize better. Learn more about ridge regression here: https://www.youtube.com/watch?v=Q81RR3yKn30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-combine",
   "metadata": {},
   "source": [
    "> Creating and training a linear regression model using the `Ridge` class from `sklearn.linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Ridge(alpha = 0.5, normalize = False, tol = 0.001,\n",
    "              solver ='auto', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using inputs and targets\n",
    "model.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-attribute",
   "metadata": {},
   "source": [
    "`model.fit` uses the following strategy for training the model (source):\n",
    "\n",
    "1. We initialize a model with random parameters (weights & biases).\n",
    "2. We pass some inputs into the model to obtain predictions.\n",
    "3. We compare the model's predictions with the actual targets using the loss function.\n",
    "4. We use an optimization technique (like least squares, gradient descent etc.) to reduce the loss by adjusting the weights & biases of the model\n",
    "5. We repeat steps 1 to 4 till the predictions from the model are good enough.\n",
    "\n",
    "<img src=\"https://www.deepnetts.com/blog/wp-content/uploads/2019/02/SupervisedLearning.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-means",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-somewhere",
   "metadata": {},
   "source": [
    "## Step 4 - Make Predictions and Evaluate Your Model\n",
    "\n",
    "The model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-magazine",
   "metadata": {},
   "source": [
    "> **QUESTION 9**: Generate predictions and compute the RMSE loss for the training and validation sets. \n",
    "> \n",
    "> *Hint*: Use the `mean_squared_error` with the argument `squared=False` to compute RMSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f766ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Mean Squared Error (MSE)\n",
    "train_rmse = np.sqrt(mean_squared_error(train_targets,train_preds, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The RMSE loss for the training set is $ {}.'.format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rmse = np.sqrt(mean_squared_error(val_targets,val_preds, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The RMSE loss for the validation set is $ {}.'.format(val_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-beginning",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Let's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-complaint",
   "metadata": {},
   "source": [
    "> **QUESTION 10**: Identify the weights (or coefficients) assigned to for different features by the model.\n",
    "> \n",
    "> *Hint:* Read [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-tamil",
   "metadata": {},
   "source": [
    "Let's create a dataframe to view the weight assigned to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df = pd.DataFrame({\n",
    "    'columns': train_inputs.columns,\n",
    "    'weight': weights\n",
    "}).sort_values('weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-collection",
   "metadata": {},
   "source": [
    "Each weight is applied to the value in a specific column of the input. Higher the weight, greater the impact of the column on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-walter",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "The model can be used to make predictions on new inputs using the following helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input(single_input):\n",
    "    input_df = pd.DataFrame([single_input])\n",
    "    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n",
    "    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n",
    "    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols].values)\n",
    "    X_input = input_df[numeric_cols + encoded_cols]\n",
    "    return model.predict(X_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = { 'MSSubClass': 20, 'MSZoning': 'RL', 'LotFrontage': 77.0, 'LotArea': 9320,\n",
    " 'Street': 'Pave', 'Alley': None, 'LotShape': 'IR1', 'LandContour': 'Lvl', 'Utilities': 'AllPub',\n",
    " 'LotConfig': 'Inside', 'LandSlope': 'Gtl', 'Neighborhood': 'NAmes', 'Condition1': 'Norm', 'Condition2': 'Norm',\n",
    " 'BldgType': '1Fam', 'HouseStyle': '1Story', 'OverallQual': 4, 'OverallCond': 5, 'YearBuilt': 1959,\n",
    " 'YearRemodAdd': 1959, 'RoofStyle': 'Gable', 'RoofMatl': 'CompShg', 'Exterior1st': 'Plywood',\n",
    " 'Exterior2nd': 'Plywood', 'MasVnrType': 'None','MasVnrArea': 0.0,'ExterQual': 'TA','ExterCond': 'TA',\n",
    " 'Foundation': 'CBlock','BsmtQual': 'TA','BsmtCond': 'TA','BsmtExposure': 'No','BsmtFinType1': 'ALQ',\n",
    " 'BsmtFinSF1': 569,'BsmtFinType2': 'Unf','BsmtFinSF2': 0,'BsmtUnfSF': 381,\n",
    " 'TotalBsmtSF': 950,'Heating': 'GasA','HeatingQC': 'Fa','CentralAir': 'Y','Electrical': 'SBrkr', '1stFlrSF': 1225,\n",
    " '2ndFlrSF': 0, 'LowQualFinSF': 0, 'GrLivArea': 1225, 'BsmtFullBath': 1, 'BsmtHalfBath': 0, 'FullBath': 1,\n",
    " 'HalfBath': 1, 'BedroomAbvGr': 3, 'KitchenAbvGr': 1,'KitchenQual': 'TA','TotRmsAbvGrd': 6,'Functional': 'Typ',\n",
    " 'Fireplaces': 0,'FireplaceQu': np.nan,'GarageType': np.nan,'GarageYrBlt': np.nan,'GarageFinish': np.nan,'GarageCars': 0,\n",
    " 'GarageArea': 0,'GarageQual': np.nan,'GarageCond': np.nan,'PavedDrive': 'Y', 'WoodDeckSF': 352, 'OpenPorchSF': 0,\n",
    " 'EnclosedPorch': 0,'3SsnPorch': 0, 'ScreenPorch': 0, 'PoolArea': 0, 'PoolQC': np.nan, 'Fence': np.nan, 'MiscFeature': 'Shed',\n",
    " 'MiscVal': 400, 'MoSold': 1, 'YrSold': 2010, 'SaleType': 'WD', 'SaleCondition': 'Normal'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price = predict_input(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The predicted sale price of the house is ${}'.format(predicted_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-drive",
   "metadata": {},
   "source": [
    "Change the values in `sample_input` above and observe the effects on the predicted price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-mineral",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "Let's save the model (along with other useful objects) to disk, so that we use it for making predictions without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_predictor = {\n",
    "    'model': model,\n",
    "    'imputer': imputer,\n",
    "    'scaler': scaler,\n",
    "    'encoder': encoder,\n",
    "    'input_cols': input_cols,\n",
    "    'target_col': target_col,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'encoded_cols': encoded_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(house_price_predictor, 'house_price_predictor.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-english",
   "metadata": {},
   "source": [
    "Congratulations on training and evaluating your first machine learning model using `scikit-learn`! Let's save our work before continuing. We'll include the saved model as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(outputs=['house_price_predictor.joblib'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-tracker",
   "metadata": {},
   "source": [
    "## Make Submission\n",
    "\n",
    "To make a submission, just execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jovian.submit('zerotogbms-a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7ba98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
